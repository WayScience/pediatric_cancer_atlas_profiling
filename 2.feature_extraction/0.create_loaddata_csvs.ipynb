{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LoadData CSVs with the paths to IC functions for analysis\n",
    "\n",
    "In this notebook, we create LoadData CSVs that contains paths to each channel per image set and associated illumination correction `npy` files per channel for CellProfiler to process. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import loaddata_utils as ld_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPC: False\n"
     ]
    }
   ],
   "source": [
    "argparse = argparse.ArgumentParser(\n",
    "    description=\"Create LoadData CSV files to run CellProfiler on the cluster\"\n",
    ")\n",
    "argparse.add_argument(\"--HPC\", action=\"store_true\", help=\"Type of compute to run on\")\n",
    "\n",
    "# Parse arguments\n",
    "args = argparse.parse_args(args=sys.argv[1:] if \"ipykernel\" not in sys.argv[0] else [])\n",
    "HPC = args.HPC\n",
    "\n",
    "print(f\"HPC: {HPC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch name to find images\n",
    "batch_name = \"Round_2_data\"\n",
    "# Set the index directory based on whether HPC is used or not\n",
    "if HPC:\n",
    "    # Path for index directory to make loaddata csvs though compute cluster (HPC)\n",
    "    index_directory = pathlib.Path(\n",
    "        f\"/scratch/alpine/jtomkinson@xsede.org/ALSF_pilot_data/{batch_name}/\"\n",
    "    )\n",
    "else:\n",
    "    # Path for index directory  to make loaddata csv locally\n",
    "    index_directory = pathlib.Path(f\"/media/18tbdrive/ALSF_pilot_data/{batch_name}/\")\n",
    "\n",
    "# Set all paths that are common to both HPC and local\n",
    "config_dir_path = pathlib.Path(\"../1.illumination_correction/config_files\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "output_csv_dir = pathlib.Path(f\"./loaddata_csvs/{batch_name}\").absolute()\n",
    "output_csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "illum_directory = pathlib.Path(\n",
    "    f\"../1.illumination_correction/illum_directory/{batch_name}\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# Find all 'Images' folders within the directory\n",
    "images_folders = list(index_directory.rglob(\"Images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LoadData CSVs with illumination functions for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR00145817_CHLA-25_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145817_CHLA-25_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145818_CHLA-25_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145818_CHLA-25_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145816_CHLA-25_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145816_CHLA-25_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145438_loaddata_with_illum.csv is created!\n",
      "The BR00145438_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_loaddata_with_illum.csv is created!\n",
      "The BR00145440_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_loaddata_with_illum.csv is created!\n",
      "The BR00145439_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145438_SH-SY5Y_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145438_SH-SY5Y_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_SH-SY5Y_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_SH-SY5Y_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_SH-SY5Y_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_SH-SY5Y_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145818_CHLA-10_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145818_CHLA-10_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145817_CHLA-10_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145817_CHLA-10_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145816_CHLA-10_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145816_CHLA-10_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_CHLA-200_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_CHLA-200_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_CHLA-200_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_CHLA-200_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145438_CHLA-200_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145438_CHLA-200_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_CHLA-10_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_CHLA-10_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_CHLA-10_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_CHLA-10_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_SK-N-DZ_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_SK-N-DZ_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_SK-N-DZ_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_SK-N-DZ_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_CHP-212_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_CHP-212_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145438_CHP-212_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145438_CHP-212_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_CHP-212_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_CHP-212_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145816_CHLA-113_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145816_CHLA-113_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145818_CHLA-113_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145818_CHLA-113_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145817_CHLA-113_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145817_CHLA-113_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_SK-N-MC_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_SK-N-MC_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_SK-N-MC_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_SK-N-MC_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145817_loaddata_with_illum.csv is created!\n",
      "The BR00145817_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145818_loaddata_with_illum.csv is created!\n",
      "The BR00145818_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145816_loaddata_with_illum.csv is created!\n",
      "The BR00145816_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145817_CHLA-218_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145817_CHLA-218_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145818_CHLA-218_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145818_CHLA-218_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145816_CHLA-218_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145816_CHLA-218_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145438_KPNYN_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145438_KPNYN_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_KPNYN_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_KPNYN_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_KPNYN_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_KPNYN_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145438_NB-1_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145438_NB-1_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_NB-1_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_NB-1_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_NB-1_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_NB-1_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145439_A673_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145439_A673_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n",
      "BR00145440_A673_Reimage_loaddata_with_illum.csv is created!\n",
      "The BR00145440_A673_Reimage_loaddata_original.csv CSV file has been removed as it does not contain the IC functions.\n"
     ]
    }
   ],
   "source": [
    "# Define the one config path to use\n",
    "config_path = config_dir_path / \"config.yml\"\n",
    "\n",
    "# Find all Plate folders\n",
    "plate_folders = list(index_directory.glob(\"Plate *\"))\n",
    "\n",
    "# Build mapping of BR00 IDs to plate number\n",
    "br00_to_plate = {}\n",
    "\n",
    "# First parse \"All Cell Lines\" folders to get BR00 IDs\n",
    "for plate_folder in plate_folders:\n",
    "    if \"All Cell Lines\" in plate_folder.name:\n",
    "        for subfolder in plate_folder.iterdir():\n",
    "            if subfolder.is_dir():\n",
    "                br00_id = subfolder.name.split(\"__\")[0]\n",
    "                plate_num = plate_folder.name.split()[1]\n",
    "                br00_to_plate[br00_id] = plate_num\n",
    "\n",
    "# Now process everything\n",
    "for plate_folder in plate_folders:\n",
    "    for subfolder in plate_folder.iterdir():\n",
    "        if not subfolder.is_dir():\n",
    "            continue\n",
    "\n",
    "        br00_id = subfolder.name.split(\"__\")[0]\n",
    "\n",
    "        if \"All Cell Lines\" in plate_folder.name:\n",
    "            # Original run\n",
    "            plate_name = f\"{br00_id}\"\n",
    "        else:\n",
    "            # Reimaged\n",
    "            parts = plate_folder.name.split()\n",
    "            plate_num = parts[1]\n",
    "            cell_line = (\n",
    "                \" \".join(parts[2:]).replace(\"Reimage\", \"\").strip().replace(\" \", \"_\")\n",
    "            )\n",
    "            plate_name = f\"{br00_id}_{cell_line}_Reimage\"\n",
    "\n",
    "        path_to_output_csv = (\n",
    "            output_csv_dir / f\"{plate_name}_loaddata_original.csv\"\n",
    "        ).absolute()\n",
    "        path_to_output_with_illum_csv = (\n",
    "            output_csv_dir / f\"{plate_name}_loaddata_with_illum.csv\"\n",
    "        ).absolute()\n",
    "        folder = subfolder.absolute()\n",
    "        plate_id = br00_id\n",
    "        illum_output_path = (illum_directory / plate_id).absolute().resolve(strict=True)\n",
    "\n",
    "        # Call the function to create the LoadData CSV\n",
    "        ld_utils.create_loaddata_illum_csv(\n",
    "            index_directory=folder / \"Images\",\n",
    "            config_path=config_path,\n",
    "            path_to_output=path_to_output_csv,\n",
    "            illum_directory=illum_output_path,\n",
    "            plate_id=plate_id,\n",
    "            illum_output_path=path_to_output_with_illum_csv,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat the re-imaged data back to their original plate and remove the original poor quality data paths\n",
    "\n",
    "All CSVs have linkage back to the IC directory and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect a list of original CSVs and identify unique plate IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 BR00 IDs: ['BR00145438', 'BR00145439', 'BR00145440', 'BR00145816', 'BR00145817', 'BR00145818']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find all CSV files in the output directory\n",
    "csv_files = list(output_csv_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Step 2: Extract unique BR00 IDs from filenames\n",
    "br00_pattern = re.compile(r\"(BR00\\d+)\")  # Regex to match 'BR00' followed by digits\n",
    "\n",
    "# Collect all matching BR00 IDs from filenames\n",
    "br00_ids = {\n",
    "    br00_pattern.search(csv_file.stem).group(1)\n",
    "    for csv_file in csv_files\n",
    "    if br00_pattern.search(csv_file.stem)\n",
    "}\n",
    "\n",
    "# Sort BR00 IDs numerically to be consistent in ordering\n",
    "br00_ids = sorted(\n",
    "    br00_ids, key=lambda x: int(x[4:])\n",
    ")  # Convert digits part to integer for numerical sorting\n",
    "\n",
    "print(f\"Found {len(br00_ids)} BR00 IDs: {br00_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track/store files and add a metadata column for if a row is re-imaged or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example DataFrame for BR00 ID: BR00145438\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName_OrigER</th>\n",
       "      <th>PathName_OrigER</th>\n",
       "      <th>Metadata_Reimaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r09c03f01p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r09c03f02p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r09c03f03p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r09c03f04p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r09c03f05p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>r08c12f05p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>r08c12f06p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>r08c12f07p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>r08c12f08p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>r08c12f09p01-ch2sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/Round_2_data/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5940 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     FileName_OrigER  \\\n",
       "0     r09c03f01p01-ch2sk1fk1fl1.tiff   \n",
       "1     r09c03f02p01-ch2sk1fk1fl1.tiff   \n",
       "2     r09c03f03p01-ch2sk1fk1fl1.tiff   \n",
       "3     r09c03f04p01-ch2sk1fk1fl1.tiff   \n",
       "4     r09c03f05p01-ch2sk1fk1fl1.tiff   \n",
       "...                              ...   \n",
       "5935  r08c12f05p01-ch2sk1fk1fl1.tiff   \n",
       "5936  r08c12f06p01-ch2sk1fk1fl1.tiff   \n",
       "5937  r08c12f07p01-ch2sk1fk1fl1.tiff   \n",
       "5938  r08c12f08p01-ch2sk1fk1fl1.tiff   \n",
       "5939  r08c12f09p01-ch2sk1fk1fl1.tiff   \n",
       "\n",
       "                                        PathName_OrigER  Metadata_Reimaged  \n",
       "0     /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "1     /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "2     /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "3     /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "4     /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "...                                                 ...                ...  \n",
       "5935  /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "5936  /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "5937  /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "5938  /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "5939  /media/18tbdrive/ALSF_pilot_data/Round_2_data/...              False  \n",
       "\n",
       "[5940 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Initialize storage to track used files and find proper column order\n",
    "br00_dataframes = {br_id: [] for br_id in br00_ids}\n",
    "used_files = set()  # Store filenames used in the process\n",
    "concat_files = []  # Track new concatenated CSV files\n",
    "\n",
    "# Load one BR00 starting CSV that will have the correct column order\n",
    "column_order = pd.read_csv(\n",
    "    pathlib.Path(f\"{output_csv_dir}/{list(br00_ids)[0]}_loaddata_with_illum.csv\"),\n",
    "    nrows=0,\n",
    ").columns.tolist()\n",
    "\n",
    "# Step 4: Add 'Metadata_Reimaged' column and group by BR00 ID\n",
    "for csv_file in csv_files:\n",
    "    filename = csv_file.stem\n",
    "    match = br00_pattern.search(filename)\n",
    "\n",
    "    if match:\n",
    "        br_id = match.group(1)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        loaddata_df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Reorder DataFrame columns to match the correct column order\n",
    "        loaddata_df = loaddata_df[\n",
    "            column_order\n",
    "        ]  # Ensure the columns are in the correct order\n",
    "\n",
    "        # Add 'Metadata_Reimaged' column based on filename\n",
    "        loaddata_df[\"Metadata_Reimaged\"] = \"Re-imaged\" in filename\n",
    "\n",
    "        # Append the DataFrame to the corresponding BR00 group\n",
    "        br00_dataframes[br_id].append(loaddata_df)\n",
    "\n",
    "        # Track this file as used\n",
    "        used_files.add(csv_file.name)\n",
    "\n",
    "# Print an example DataFrame (first BR00 group)\n",
    "example_id = next(iter(br00_dataframes))  # Get the first BR00 ID\n",
    "example_df = pd.concat(br00_dataframes[example_id], ignore_index=True)\n",
    "print(f\"\\nExample DataFrame for BR00 ID: {example_id}\")\n",
    "example_df.iloc[:, [0, 1, -1]]  # Display only the first two and last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the re-imaged and original data for the same plate and remove any duplicate wells that come from the original data\n",
    "\n",
    "We remove the duplicates that aren't re-imaged since they are of poor quality. We want to analyze the re-imaged data from those same wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All image path columns start correctly for BR00145440_A673_Reimage\n",
      "Saved: /home/jenna/pediatric_cancer_atlas_profiling/2.feature_extraction/loaddata_csvs/Round_2_data/BR00145438_concatenated_with_illum.csv\n",
      "All image path columns start correctly for BR00145440_A673_Reimage\n",
      "Saved: /home/jenna/pediatric_cancer_atlas_profiling/2.feature_extraction/loaddata_csvs/Round_2_data/BR00145439_concatenated_with_illum.csv\n",
      "All image path columns start correctly for BR00145440_A673_Reimage\n",
      "Saved: /home/jenna/pediatric_cancer_atlas_profiling/2.feature_extraction/loaddata_csvs/Round_2_data/BR00145440_concatenated_with_illum.csv\n",
      "All image path columns start correctly for BR00145440_A673_Reimage\n",
      "Saved: /home/jenna/pediatric_cancer_atlas_profiling/2.feature_extraction/loaddata_csvs/Round_2_data/BR00145816_concatenated_with_illum.csv\n",
      "All image path columns start correctly for BR00145440_A673_Reimage\n",
      "Saved: /home/jenna/pediatric_cancer_atlas_profiling/2.feature_extraction/loaddata_csvs/Round_2_data/BR00145817_concatenated_with_illum.csv\n",
      "All image path columns start correctly for BR00145440_A673_Reimage\n",
      "Saved: /home/jenna/pediatric_cancer_atlas_profiling/2.feature_extraction/loaddata_csvs/Round_2_data/BR00145818_concatenated_with_illum.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Concatenate DataFrames, drop duplicates, and save per BR00 ID\n",
    "for br_id, dfs in br00_dataframes.items():\n",
    "    if dfs:  # Only process if there are matching files\n",
    "        concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Drop duplicates, prioritizing rows with 'Metadata_Reimaged' == True\n",
    "        deduplicated_df = concatenated_df.sort_values(\n",
    "            \"Metadata_Reimaged\", ascending=False\n",
    "        ).drop_duplicates(subset=[\"Metadata_Well\", \"Metadata_Site\"], keep=\"first\")\n",
    "\n",
    "        # Sort by 'Metadata_Col', 'Metadata_Row', and 'Metadata_Site\n",
    "        sorted_df = deduplicated_df.sort_values(\n",
    "            [\"Metadata_Col\", \"Metadata_Row\", \"Metadata_Site\"], ascending=True\n",
    "        )\n",
    "\n",
    "        # Enforce correct plate ID for all rows\n",
    "        sorted_df[\"Metadata_Plate\"] = br_id\n",
    "\n",
    "        # Define the expected base path for images\n",
    "        expected_base_path = str(index_directory.resolve())\n",
    "\n",
    "        # List of columns that contain image paths (not illum functions)\n",
    "        image_path_columns = [\n",
    "            col\n",
    "            for col in loaddata_df.columns\n",
    "            if col.startswith(\"PathName_\") and \"Illum\" not in col\n",
    "        ]\n",
    "\n",
    "        # Sanity check: Ensure all image paths start with the expected base path\n",
    "        warning_found = False\n",
    "        for col in image_path_columns:\n",
    "            if (\n",
    "                not loaddata_df[col]\n",
    "                .astype(str)\n",
    "                .str.startswith(expected_base_path)\n",
    "                .all()\n",
    "            ):\n",
    "                print(\n",
    "                    f\"Warning: Not all paths in column '{col}' start with '{expected_base_path}'\"\n",
    "                )\n",
    "                warning_found = True\n",
    "\n",
    "        if not warning_found:\n",
    "            print(f\"All image path columns start correctly for {plate_name}\")\n",
    "\n",
    "        # Save the cleaned, concatenated, and sorted DataFrame to a new CSV file\n",
    "        output_path = output_csv_dir / f\"{br_id}_concatenated_with_illum.csv\"\n",
    "        sorted_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Saved: {output_path}\")\n",
    "        concat_files.append(output_path)  # Track new concatenated files\n",
    "    else:\n",
    "        print(f\"No files found for {br_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that all LoadData CSV files were included in previous concat (avoid data loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files were successfully used.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Verify all files were used\n",
    "unused_files = set(csv_file.name for csv_file in csv_files) - used_files\n",
    "\n",
    "if unused_files:\n",
    "    print(\"Warning: Some files were not used in the concatenation!\")\n",
    "    for file in unused_files:\n",
    "        print(f\"Unused: {file}\")\n",
    "else:\n",
    "    print(\"All files were successfully used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the original CSV files to prevent CellProfiler from using them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: BR00145440_NB-1_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145440_A673_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_CHLA-10_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_NB-1_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145817_loaddata_with_illum.csv\n",
      "Removed: BR00145439_SH-SY5Y_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_CHLA-200_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145440_SK-N-DZ_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145438_NB-1_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_SK-N-DZ_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145817_CHLA-25_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145818_loaddata_with_illum.csv\n",
      "Removed: BR00145440_SK-N-MC_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145440_loaddata_with_illum.csv\n",
      "Removed: BR00145440_CHLA-10_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145818_CHLA-113_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145817_CHLA-218_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_CHP-212_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145440_SH-SY5Y_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145440_CHP-212_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145817_CHLA-10_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145816_CHLA-25_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145816_loaddata_with_illum.csv\n",
      "Removed: BR00145438_KPNYN_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_A673_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145816_CHLA-10_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145438_loaddata_with_illum.csv\n",
      "Removed: BR00145817_CHLA-113_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145438_SH-SY5Y_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145816_CHLA-218_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145438_CHLA-200_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145438_CHP-212_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145816_CHLA-113_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145818_CHLA-25_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_KPNYN_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_SK-N-MC_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145439_loaddata_with_illum.csv\n",
      "Removed: BR00145440_KPNYN_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145818_CHLA-10_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145440_CHLA-200_Reimage_loaddata_with_illum.csv\n",
      "Removed: BR00145818_CHLA-218_Reimage_loaddata_with_illum.csv\n",
      "All non-concatenated CSV files have been removed.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Remove all non-concatenated CSVs to avoid confusion\n",
    "for csv_file in csv_files:\n",
    "    if csv_file not in concat_files:  # Keep only new concatenated files\n",
    "        csv_file.unlink()  # Delete the file\n",
    "        print(f\"Removed: {csv_file.name}\")\n",
    "print(\"All non-concatenated CSV files have been removed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alsf_cp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
